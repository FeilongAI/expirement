import numpy as np

# 读取Autoformer预测值和真实值
autoformer = [20.75255, 19.86634, 20.42124, 20.1412, 20.67214, 20.6737, 23.03569, 23.13281, 24.27856, 25.72047, 25.52086, 28.11273, 25.13084, 23.46667, 25.12135, 26.69332, 28.24418, 31.27913, 31.80717, 32.1002, 32.10568, 30.34929, 30.35663, 31.58652, 33.20348, 38.16155, 38.60929, 40.27152, 36.56428, 32.92718, 28.10452, 22.14053, 22.39436, 22.35487, 23.05598, 21.17735, 20.24036, 19.17332, 17.58312, 16.17668, 15.54757, 16.55063, 16.50244, 16.74461, 17.76109, 18.66826, 19.00619, 18.55693, 17.40721, 16.92934, 15.27945, 15.50742, 15.79971, 15.95679, 16.06942, 15.92096, 15.87016, 15.34944, 15.47713, 16.74734, 17.58382, 18.01806, 17.22648, 19.5011, 19.71441, 19.10073, 17.93737, 19.82245, 20.44669, 19.48099, 18.70116, 18.00244, 17.36392, 16.57551, 16.44705, 16.43399, 16.50674, 16.67953, 17.11568, 17.25857, 17.35676, 17.41141, 17.60554, 17.93009, 18.6181, 18.49977, 18.13343, 17.56179, 16.64174, 16.43317, 16.57003, 15.29536, 15.44399, 15.59724, 15.7057, 15.64175, 17.04199, 16.93947, 17.13924, 17.78087, 19.14587, 19.71391, 19.72623, 19.3692, 20.42826, 20.98722, 21.88815, 20.74113, 21.99879, 21.11162, 19.57621, 19.33741, 18.97102, 18.59298, 18.75167, 19.31644, 19.97367, 19.97699, 19.8781, 19.69044, 20.01263, 20.63532, 21.27722, 21.48226, 21.57999, 21.49529, 22.746, 22.42786, 23.7689, 26.05822, 30.89759, 28.64365, 28.13897, 24.28587, 23.93585, 26.38046, 23.71453, 22.9634, 21.89188, 21.6078, 19.62485, 20.73587, 20.17676, 20.28119, 18.3108, 20.56992, 20.13146, 19.86987, 19.50184, 18.87239, 18.63865, 18.55114, 18.71455, 15.78981, 16.0568, 16.49169, 17.60352, 17.14199, 19.53139, 16.71101, 17.28676, 16.41144, 16.40953, 16.29435, 16.36056, 16.33982, 16.50296, 16.55821, 17.10223, 16.27096, 16.06045, 16.49877, 16.55269, 16.10427, 16.71707, 17.38489, 18.15734, 17.98079, 17.63334, 17.49933, 16.49887, 16.16132, 16.75117, 18.30508, 19.61484, 19.53241, 18.88642, 18.31256, 17.24251, 16.9473, 19.9659, 19.92297, 20.15895, 20.68755, 21.14487, 20.27023, 18.52768, 18.5788, 17.32418, 17.28654]

true_values = [28, 31, 28, 27, 26, 31, 33, 33, 33, 35, 25, 23, 28, 29, 34, 39, 42, 47, 41, 37, 31, 36, 34, 36, 42, 37, 48, 52, 45, 27, 26, 26, 27, 25, 21, 17, 13, 11, 9, 8, 7, 14, 24, 33, 26, 19, 14, 14, 15, 14, 15, 15, 14, 13, 12, 11, 12, 13, 13, 19, 20, 19, 18, 19, 20, 25, 22, 19, 15, 13, 10, 8, 6, 8, 13, 15, 16, 13, 15, 17, 18, 17, 17, 16, 13, 11, 9, 9, 10, 9, 9, 10, 11, 10, 10, 9, 10, 13, 20, 25, 25, 27, 26, 22, 21, 28, 36, 34, 37, 40, 20, 22, 24, 21, 22, 25, 28, 20, 18, 20, 22, 24, 20, 20, 25, 25, 24, 26, 27, 27, 25, 25, 30, 37, 27, 35, 28, 23, 23, 26, 25, 23, 26, 26, 20, 20, 20, 15, 19, 24, 28, 24, 22, 18, 16, 15, 14, 13, 11, 10, 10, 10, 12, 11, 12, 10, 8, 7, 8, 8, 9, 11, 13, 12, 10, 9, 9, 10, 11, 12, 16, 15, 11, 9, 8, 9, 11, 12, 16, 17, 12, 9, 9, 10, 12, 13, 25, 18, 18, 16]

np.random.seed(42)

def add_noise(values, scale):
    return np.clip(np.array(values) + np.random.normal(0, scale, len(values)), 0, None).tolist()

# 高性能模型：结合Autoformer和真实值的优点
high_performance = [(0.8 * a + 0.2 * t) for a, t in zip(autoformer, true_values)]
high_performance = add_noise(high_performance, 0.5)

# Informer：类似Autoformer，但对长序列处理更好
informer = add_noise(autoformer, 1.5)

# GRU：能捕捉中期依赖，但可能会错过一些长期模式
gru = add_noise(true_values[5:] + true_values[:5], 3)

# Transformer：善于捕捉长期依赖，但可能过度拟合
transformer = add_noise([(0.7 * a + 0.3 * t) for a, t in zip(autoformer, true_values)], 2)

# LSTM：善于捕捉长期依赖，但可能会有一些滞后
lstm = add_noise(true_values[3:] + true_values[:3], 2.5)

# MLP：可能会错过一些时间依赖性
mlp = add_noise([np.mean(true_values[max(0, i-5):i+1]) for i in range(len(true_values))], 4)

print("High Performance Model predictions:")
print(high_performance)
print("\nInformer predictions:")
print(informer)
print("\nGRU predictions:")
print(gru)
print("\nTransformer predictions:")
print(transformer)
print("\nLSTM predictions:")
print(lstm)
print("\nMLP predictions:")
print(mlp)

# 模型特性说明
model_characteristics = """
1. High Performance Model:
   - 结合了Autoformer的预测能力和真实值的部分信息
   - 加入了少量噪声以模拟现实世界的不确定性
   - 预期性能最好，误差最小

2. Informer:
   - 类似Autoformer，但对长序列处理更好
   - 添加了较小的随机噪声以区别于Autoformer
   - 预期性能仅次于高性能模型

3. GRU (Gated Recurrent Unit):
   - 能够捕捉中期依赖关系
   - 可能会错过一些长期模式
   - 模拟了轻微的时间滞后（5个时间步）
   - 加入了中等程度的噪声

4. Transformer:
   - 善于捕捉长期依赖关系
   - 结合了Autoformer和真实值的信息，但权重不同
   - 加入了一定程度的噪声以模拟过拟合的可能性

5. LSTM (Long Short-Term Memory):
   - 善于捕捉长期依赖关系
   - 模拟了轻微的时间滞后（3个时间步）
   - 加入了中等程度的噪声
   - 预期性能介于GRU和Transformer之间

6. MLP (Multi-Layer Perceptron):
   - 使用简单的移动平均作为基础预测
   - 加入了较大的随机噪声
   - 可能会错过一些时间依赖性
   - 预期性能最差

注意：这些预测值是基于模型特性的模拟，并不代表实际训练的模型结果。在实际应用中，各模型的性能可能会因数据特性和具体实现而有所不同。
"""

print(model_characteristics)
